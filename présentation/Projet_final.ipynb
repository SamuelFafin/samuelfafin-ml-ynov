{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Présentation\n",
    "## 1.1 - Introduction\n",
    "Les réseaux sociaux sont une source de données presques infinis. Avec les Tweets sur Twitter ou les murs sur Facebook, il y a une multitude d'informations en attente d'être étudiées.\n",
    "\n",
    "\n",
    "Le projet est une application qui permet de lire les tweets sur twitter. Durant cette lecture, le programme va définir si un message est positif ou négatif. Ce qui va permettre de définir une réputation.\n",
    "\n",
    "L'objectif de ce projet est de créer un programme qui comprends si un tweet est positif ou négatif. Cela peut servir pour définir la réputation d'une entreprise sur les réseaux sociaux.\n",
    "\n",
    "Le but de ce jeu de données est de prédire si un tweet est positif, neutre ou négatif. L'exercice nécessite de réaliser une analyse de sentiment en réalisant une classification à 3 sorties.\n",
    "\n",
    "## 1.2 - Source de Données\n",
    "La source de données est disponible via le lien suivant : http://help.sentiment140.com/for-students/\n",
    "\n",
    "Les données sont des tweets avec divers métadata pour compléter l'information.\n",
    "\n",
    "# 2 - Préparation des données\n",
    "## 2.1 - Charger les librairies\n",
    "Ci-dessous la liste des librairies nécessaires durant l'analyse des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Définir la liste des librairies nécessaire pour les différentes\n",
    "# étapes du traitement\n",
    "####################\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Chargement du fichier d'apprentissage d'entraintement\n",
    "####################\n",
    "\n",
    "dataSource = pd.read_csv('./data/training.csv', \n",
    "                 header=0,\n",
    "                 sep=',',\n",
    "                 names=['Sentimental','Id','Date','Query', 'Owner','Tweet'],\n",
    "#                 nrows = 5000000,\n",
    "                 encoding = 'latin-1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous allons afficher les 20 premières lignes de ce dataset pour mieux appréhender l'information :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Découverte des données\n",
    "####################\n",
    "\n",
    "# Charger 10000 tweets négatifs\n",
    "dataNegative = dataSource[-5000:]\n",
    "\n",
    "# Charger 10000 tweets positifs\n",
    "dataPositive = dataSource[:5000]\n",
    " \n",
    "# Concatener les deux éléments à fin d'en créer un seul\n",
    "df = pd.concat([dataPositive, dataNegative])\n",
    "\n",
    "# Afficher les 20 premières lignes\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il  y a 6 dimensions dans ce dataset :\n",
    "* Le sentiment du tweet (0 = negatif, 2 = neutre et 4 = positif) ;\n",
    "* L'id du tweet ;\n",
    "* La date de publication ;\n",
    "* La requête. (S'il n'y en a aucune alors NO_QUERY) ;\n",
    "* L'utilisateur qui a tweeté ;\n",
    "* Le texte du tweet.\n",
    "\n",
    "Nous allons afficher les types des données chargées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les types des données sont corrects pour l'analyse. Il n'est pas nécessaire de les casts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Nettoyer les données\n",
    "Dans l'objectif d'analyser les données, il faut tout d'abord les nettoyer afin qu'elles soient le plus facile à lire pour l'algorithme.\n",
    "\n",
    "Durant cette partie nous allons définir des règles pour le nettoyage de données non structurée au format texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Nettoyer les données\n",
    "####################\n",
    "\n",
    "# Créer une fonction qui va appliquer toutes les règles de nettoyage\n",
    "def dataClean(column):\n",
    "    # Définir la ponctuation à supprimer\n",
    "    signs = ['.', ',' ,'!', '?', ';', \"'\", '-', '_', '&', '(', ')', '*']\n",
    "    \n",
    "    # Transformer les caractères en minuscule\n",
    "    column = column.str.lower()\n",
    "    # Supprimer les utilisateurs\n",
    "    column = column.str.replace('@[\\w]*', '')\n",
    "    # Supprimer les tags\n",
    "    column = column.str.replace('#[\\w]*', '')\n",
    "    \n",
    "    # Supprimer les signes\n",
    "    for sign in signs:\n",
    "        column = column.str.replace(sign, '')\n",
    "    \n",
    "    # Supprimer les liens hypertextes\n",
    "    column = column.str.replace('/^(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?$/', '')\n",
    "    # Supprimer les espaces en début et fin de mot\n",
    "    column = column.str.strip()\n",
    "    \n",
    "    return column\n",
    "\n",
    "# Appliquer la fonction de nettoyage aux données\n",
    "df['Tweet'] = dataClean(df['Tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cette fonction va permettre de préparer les données pour la suite de l'analyse.\n",
    "\n",
    "Les actions réalisées :\n",
    "* Mettre le texte en minuscule afin d'éviter de compter certains mots en double comme par exemple : Hasard et hasard. C'est le même mot pour nous en le lisant mais pour l'algorithme cela est différent ;\n",
    "* Supprimer les pseudos Twitter ;\n",
    "* Supprimer les caractères comme le \".\" ou le \"!\" ;\n",
    "* Supprimer les URL ;\n",
    "* Supprimer les espaces avant et après le paragraphe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Preprocessing\n",
    "Maintenant que nous allons nettoyer les données, il faut adapter l'information afin de la rendre exploitable par les algorithmes de Machine Learning.\n",
    "\n",
    "<span style=\"color:orange\">/!\\ Lancer ces commandes si c'est la première fois que vous utilisez NLTK :\n",
    "* <span style=\"color:orange\">nltk.download('punkt')</span>\n",
    "* <span style=\"color:orange\">nltk.download(\"stopwords\")</span>\n",
    "* <span style=\"color:orange\">nltk.download('wordnet')</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Transformer les données\n",
    "####################\n",
    "\n",
    "# Définir la fonction de préprocessing\n",
    "def preprocessing(column):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # Tokenizer les tweets\n",
    "    column = column.apply(lambda row: nltk.word_tokenize(row))\n",
    "    # Supprimer les stopwords\n",
    "    column = column.apply(lambda x: [item for item in x if item not in stopwords.words('english')])\n",
    "    # Appliquer le lemmatizer\n",
    "    column = column.apply(lambda x: [wordnet_lemmatizer.lemmatize(item) for item in x])\n",
    "\n",
    "    return column\n",
    "\n",
    "# Appliquer la fonction sur les données\n",
    "df['TweetTK'] = preprocessing(df['Tweet'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction va permettre de **Tokenize** les tweets, faire disparaitre les **Stop Words** puis d'appliquer un algorithme **Lemmatisation**.\n",
    "\n",
    "## 2.5 - Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Transformer les données\n",
    "####################\n",
    "\n",
    "# Définir une fonction qui permet de créer un tableau\n",
    "# qui contient chaque mot de chaque tweet\n",
    "def getListofWords(tweets):\n",
    "    allWords = []\n",
    "    for words in tweets:\n",
    "        allWords.extend(words)\n",
    "    return allWords\n",
    "\n",
    "# Définir une fonction qui supprimer tout les doublons d'un tableau\n",
    "def getWordFeatures(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "def findFeatures(document, wordFeatures):\n",
    "    features = {}\n",
    "    for w in wordFeatures:\n",
    "        features[w] = (w in document)\n",
    "    return features\n",
    "\n",
    "listWords = getListofWords(df['TweetTK'])\n",
    "wordFeatures = getWordFeatures(listWords)\n",
    "df['TweetFeatures'] = df['TweetTK'].apply(lambda x: findFeatures(x, wordFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 - Conclusion\n",
    "Les données sont maintenant exploitables pour commencer l'étape d'apprentissage en Machine Learning.\n",
    "\n",
    "# 3. Apprentissage\n",
    "Dans cette partie nous allons apprendre à l'algorithme NaiveBayes à reconnaitre si un tweet est positif ou négatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Phrase d'apprentissage de l'algorithme \n",
    "####################\n",
    "\n",
    "trainingSet = []\n",
    "\n",
    "for words, sentimental in zip(df['TweetFeatures'], df['Sentimental']):\n",
    "     for k, v in zip(words, words.values()):\n",
    "        trainingSet.append(({k: v}, sentimental))\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(trainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. Vérification\n",
    "Dans cette partie, nous allons vérifier la qualité de l'apprentissage sur un jeu de données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Phrase de test de l'algorithme \n",
    "####################\n",
    "\n",
    "datasetTest = pd.read_csv('./data/test.csv', \n",
    "                 header=0,\n",
    "                 sep=',',\n",
    "                 names=['Sentimental','Id','Date','Query', 'Owner','Tweet'],\n",
    "                 encoding = 'latin-1'\n",
    ")\n",
    "\n",
    "datasetTest['Tweet'] = dataClean(datasetTest['Tweet'])\n",
    "datasetTest['TweetTK'] = preprocessing(datasetTest['Tweet'])\n",
    "\n",
    "listWords = getListofWords(datasetTest['TweetTK'])\n",
    "wordFeatures = getWordFeatures(listWords)\n",
    "datasetTest['TweetFeatures'] = datasetTest['TweetTK'].apply(lambda x: findFeatures(x, wordFeatures))\n",
    "\n",
    "testSet = []\n",
    "\n",
    "for words, sentimental in zip(datasetTest['TweetFeatures'], datasetTest['Sentimental']):\n",
    "    for k, v in zip(words, words.values()):\n",
    "        testSet.append(({k: v}, sentimental))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant vérifier le résultat sur un dataset de test pour connaitre le % de bonne réponses.\n",
    "\n",
    "La fonction ci-dessous permet de tester la véracité de l'apprentissage avec les données de tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Permettre de vérifier l'apprentissage avec les données de tests tout en mesurant\n",
    "# les performances via un pourcentage.\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testSet))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Le taux de bonnes réponses est d'environ 36% avec 20000 tweets.\n",
    "\n",
    "Malheureusement l'ordinateur qui fait tourner ce script ne peut prendre en charge les 1 600 000 tweets de la base de tests avec cet algorithme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de ML Naive Bayes fonctionne avec un taux de 36% en moyenne de bonnes réponses sur le dataset de test. Malheureusement je ne peux charger que 20000 lignes du dataset d'entrainement. Mon ordinateur ne tient pas la charge. Je ne peux donc pas vérifier si je peux atteindre un taux de réussite plus important avec cette méthodologie.\n",
    "\n",
    "\n",
    "# 6. Continuation\n",
    "La continuation du projet serait de trouver pourquoi nous avons un taux de réussite aussi faible et d'améliorer le modèle.\n",
    "\n",
    "Une autre piste est d'améliorer le code afin d'essayer de mieux gérer la ram afin de pouvoir prendre plus de lignes dans le traininset.\n",
    "\n",
    "\n",
    "# 7. Notation Personnelle\n",
    "\n",
    "Je me donnerai une note entre 13 et 14 pour le rendu effectué.\n",
    "\n",
    "Cette notation comprends notamment le nettoyage des données, le preprocessing, le bag of words ainsi que l'utilisation de l'algorithme de machine learning Nayves Baye.\n",
    "\n",
    "Je n'ai malheureusent pas réussir à obtenir un résultat satisfaisant pour le machine learning avec un taux de 36% de bonnes réponses sur le dataset de test. Ce chiffre restre très faible et n'est malheureusement pas satisfaisant pour une application dans un environnement de production.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
