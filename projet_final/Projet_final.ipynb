{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Présentation\n",
    "## 1.1 - Introduction\n",
    "Les réseaux sociaux sont une source de données presques infinis. Avec les Tweets sur Twitter ou les murs sur Facebook, il y a une multitude d'informations en attente d'être étudiées.\n",
    "\n",
    "L'objectif de ce jeu de données est de prédire si un tweet est positif ou négatif. L'exercice nécessite de réaliser une analyse de sentiment en réalisant une classification à 2 sorties.\n",
    "\n",
    "## 1.2 - Source de Données\n",
    "Les données sont représentés par 2 dimensions :\n",
    "* un chiffre de 0 ou 4 qui représente si un tweet est positif(4) ou négatif(0) ;\n",
    "* une chaine de caractères représentant le tweet.\n",
    "\n",
    "# 2 - Préparation des données\n",
    "## 2.1 - Charger les librairies\n",
    "Ci-dessous la liste des librairies nécessaires durant l'analyse des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataSource = pd.read_csv('./data/training.csv', \n",
    "                 header=0,\n",
    "                 sep=',',\n",
    "                 names=['Sentimental','Id','Date','Query', 'Owner','Tweet'],\n",
    "#                 nrows = 5000000,\n",
    "                 encoding = 'latin-1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous allons afficher les 20 premières lignes de ce dataset pour mieux appréhender l'information :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataNegative = dataSource[-10000:]\n",
    "dataPositive = dataSource[:10000]\n",
    " \n",
    "df = pd.concat([dataPositive, dataNegative])\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il  y a 6 dimensions dans ce dataset :\n",
    "* Le sentiment du tweet (0 = negatif, 2 = neutre et 4 = positif) ;\n",
    "* L'id du tweet ;\n",
    "* La date de publication ;\n",
    "* La requête. (S'il n'y en a aucune alors NO_QUERY) ;\n",
    "* L'utilisateur qui a tweeté ;\n",
    "* Le texte du tweet.\n",
    "\n",
    "Nous allons afficher les types des données chargées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les types des données sont corrects pour l'analyse. Il n'est pas nécessaire de les casts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Nettoyer les données\n",
    "Dans l'objectif d'analyser les données, il faut tout d'abord les nettoyer afin qu'elles soient plus facile à lire pour la machine.\n",
    "\n",
    "Durant cette partie nous allons définir des règles pour le nettoyage de données non structurée au format texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataClean(column):\n",
    "    signs = ['.', ',' ,'!', '?', ';', \"'\", '-', '_', '&', '(', ')', '*']\n",
    "    \n",
    "    column = column.str.lower()\n",
    "    column = column.str.replace('@[\\w]*', '')\n",
    "    column = column.str.replace('#[\\w]*', '')\n",
    "    \n",
    "    for sign in signs:\n",
    "        column = column.str.replace(sign, '')\n",
    "    \n",
    "    column = column.str.replace('/^(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?$/', '')\n",
    "    column = column.str.strip()\n",
    "    \n",
    "    return column\n",
    "    \n",
    "df['Tweet'] = dataClean(df['Tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cette fonction va permettre de préparer les données pour la suite de l'analyse.\n",
    "\n",
    "Les actions réalisées :\n",
    "* Mettre le texte en minuscule afin d'éviter de compter certains mots en double comme par exemple : Hasard et hasard. C'est le même mot pour nous en le lisant mais pour l'algorithme cela est différent ;\n",
    "* Supprimer les pseudos Twitter ;\n",
    "* Supprimer les caractères comme le \".\" ou le \"!\" ;\n",
    "* Supprimer les URL ;\n",
    "* Supprimer les espaces avant et après le paragraphe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Preprocessing\n",
    "Maintenant que nous avons nettoyer les données, il faut adapter l'information afin de la rendre exploitable par les algorithmes de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessing(column):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    column = column.apply(lambda row: nltk.word_tokenize(row))\n",
    "    column = column.apply(lambda x: [item for item in x if item not in stopwords.words('english')])\n",
    "    column = column.apply(lambda x: [wordnet_lemmatizer.lemmatize(item) for item in x])\n",
    "\n",
    "    return column\n",
    "df['TweetTK'] = preprocessing(df['Tweet'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction va permettre de **Tokenize** les tweets, faire disparaitre les **Stop Words** puis d'appliquer un algorithme **Lemmatisation**.\n",
    "\n",
    "## 2.5 - Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getListofWords(tweets):\n",
    "    allWords = []\n",
    "    for words in tweets:\n",
    "        allWords.extend(words)\n",
    "    return allWords\n",
    "\n",
    "def getWordFeatures(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "def findFeatures(document, wordFeatures):\n",
    "    features = {}\n",
    "    for w in wordFeatures:\n",
    "        features[w] = (w in document)\n",
    "    return features\n",
    "\n",
    "listWords = getListofWords(df['TweetTK'])\n",
    "wordFeatures = getWordFeatures(listWords)\n",
    "df['TweetFeatures'] = df['TweetTK'].apply(lambda x: findFeatures(x, wordFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 - Conclusion\n",
    "Les données sont maintenant exploitables pour commencer l'étape d'apprentissage en Machine Learning.\n",
    "\n",
    "# 3. Apprentissage\n",
    "Dans cette partie nous allons apprendre à l'algorithme NaiveBayes à reconnaitre si un tweet est positif ou négatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingSet = []\n",
    "\n",
    "for words, sentimental in zip(df['TweetTK'], df['Sentimental']):\n",
    "    for word in words:\n",
    "        trainingSet.append(({'word': word}, sentimental))\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(trainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. Vérification\n",
    "Dans cette partie, nous allons vérifier la qualité de l'apprentissage sur un jeu de données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasetTest = pd.read_csv('./data/test.csv', \n",
    "                 header=0,\n",
    "                 sep=',',\n",
    "                 names=['Sentimental','Id','Date','Query', 'Owner','Tweet'],\n",
    "                 encoding = 'latin-1'\n",
    ")\n",
    "\n",
    "datasetTest['Tweet'] = dataClean(datasetTest['Tweet'])\n",
    "datasetTest['TweetTK'] = preprocessing(datasetTest['Tweet'])\n",
    "\n",
    "datasetTest.head()\n",
    "\n",
    "listWords = getListofWords(datasetTest['TweetTK'])\n",
    "wordFeatures = getWordFeatures(listWords)\n",
    "datasetTest['TweetFeatures'] = datasetTest['TweetTK'].apply(lambda x: findFeatures(x, wordFeatures))\n",
    "\n",
    "testSet = []\n",
    "\n",
    "for words, sentimental in zip(datasetTest['TweetTK'], datasetTest['Sentimental']):\n",
    "    for word in words:\n",
    "        testSet.append(({'word': word}, sentimental))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant vérifier le résultat sur un dataset de test pour connaitre le % de bonne réponses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Le taux de bonnes réponses est d'environ 42% avec 20000 tweets. Malheureusement l'ordinateur qui fait tourner se script ne peut pas prendre en charge les 1 600 000 tweets de la base de tests avec cet algorithme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testSet\n",
    "\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testSet))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de ML Naive Bayes à l'air de fonctionner malheureusement, il est possible d'utiliser le dataset complet pour l'apprentissage et voir le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
