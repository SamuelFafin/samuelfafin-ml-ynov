{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Présentation\n",
    "## 1.1 - Introduction\n",
    "Les réseaux sociaux sont une source de données presques infinis. Avec les Tweets sur Twitter ou les murs sur Facebook, il y a une multitude d'informations en attente d'être étudiées.\n",
    "\n",
    "L'objectif de ce jeu de données est de prédire si un tweet est positif ou négatif. L'exercice nécessite de réaliser une analyse de sentiment en réalisant une classification à 2 sorties.\n",
    "\n",
    "## 1.2 - Source de Données\n",
    "Les données sont représentés par 2 dimensions :\n",
    "* un chiffre de 0 ou 1 qui représente si un tweet est positif(1) ou négatif(0) ;\n",
    "* une chaine de caractères représentant le tweet.\n",
    "\n",
    "# 2 - Préparation des données\n",
    "## 2.1 - Charger les librairies\n",
    "Ci-dessous la liste des librairies nécessaires durant l'analyse des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/training.csv', \n",
    "                 header=0,\n",
    "                 sep=',',\n",
    "                 names=['Sentimental','Id','Date','Query', 'Owner','Tweet'],\n",
    "                 nrows = 1000,\n",
    "                 encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous allons afficher les 20 premières lignes de ce dataset pour mieux appréhender l'information :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il  y a 6 dimensions dans ce dataset :\n",
    "* Le sentiment du tweet (0 = negatif, 2 = neutre et 4 = positif) ;\n",
    "* L'id du tweet ;\n",
    "* La date de publication ;\n",
    "* La requête. (S'il n'y en a aucune alors NO_QUERY) ;\n",
    "* L'utilisateur qui a tweeté ;\n",
    "* Le texte du tweet.\n",
    "\n",
    "Nous allons afficher les types des données chargées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les types des données sont corrects pour l'analyse. Il n'est pas nécessaire de les casts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Nettoyer les données\n",
    "Avant de passer à l'analyse nous allons nettoyer les données.\n",
    "\n",
    "### 2.3.1 - Minuscule\n",
    "Nous allons enlever les majuscules des différents tweets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Tweet'] = df['Tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cela va permettre de ne pas avoir de mots doublons pour la suite de l'analyse. Comme par exemple \"awesome\" et \"Awesome\" qui auraient été analysés comme des mots différents.\n",
    "\n",
    "### 2.3.2 - @name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Tweet'] = df['Tweet'].str.replace('@[\\w]*', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 - Ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "signs = ['.', ',' ,'!', '?', ';', \"'\", '-', '_', '&', '(', ')', '*']\n",
    "\n",
    "for sign in signs:\n",
    "    df['Tweet'] = df['Tweet'].str.replace(sign, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Strip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Tweet'] = df['Tweet'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Transformation\n",
    "### 2.4.1 - Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['TweetTK'] = df.apply(lambda row: nltk.word_tokenize(row['Tweet']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 - Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['TweetTK'] = df['TweetTK'].apply(lambda x: [item for item in x if item not in stopwords.words('english')])\n",
    "df['TweetTK'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.4.3 - Lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "df['TweetTK'] = df['TweetTK'].apply(lambda x: [wordnet_lemmatizer.lemmatize(item) for item in x])\n",
    "\n",
    "df['TweetTK'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Conclusion\n",
    "# 3. Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "def find_features(document):\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "tweets = []\n",
    "for x, y in zip(df['TweetTK'], df['Sentimental']):\n",
    "    tweets.append((x, y))\n",
    "\n",
    "\n",
    "word_features = get_word_features(get_words_in_tweets(tweets))\n",
    "word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    print(document)\n",
    "    features = {}\n",
    "    for word in tweet:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set = nltk.classify.apply_features(extract_features, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
